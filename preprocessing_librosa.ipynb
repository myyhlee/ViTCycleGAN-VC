{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리부분 #! 할것! 멜 노말라이즈\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from librosa.filters import mel as librosa_mel_fn # 멜 생성\n",
    "import soundfile as sf\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import cv2\n",
    "import torchaudio\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "vocoder = torch.hub.load('descriptinc/melgan-neurips', 'load_melgan')\n",
    "\n",
    "def load_pickle_file(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "        \n",
    "def save_pickle_file(variable, fileName):\n",
    "    with open(fileName, 'wb') as f:\n",
    "        pickle.dump(variable, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #############################################################\n",
    "            #################                           #################\n",
    "            #############################################################\n",
    "\n",
    "\n",
    "\n",
    "def wave2mel_melgan(wavspath, SAMPLING_RATE=22050): #! 아직 노말라이징은 없음\n",
    "    wav_files = glob(os.path.join(\n",
    "        wavspath, '**', '*.wav'), recursive=True)\n",
    "    \n",
    "    wav_files = sorted(wav_files)\n",
    "\n",
    "    print(len(wav_files))\n",
    "    print(wav_files)\n",
    "\n",
    "    mel_list = []\n",
    "\n",
    "    for wav_path in tqdm(wav_files, desc='Preprocessing wav to mel (MelGAN)'): \n",
    "        wave_ori, _ = librosa.load(wav_path, sr=SAMPLING_RATE, mono=True)\n",
    "        \n",
    "        spectrogram = vocoder(torch.tensor([wave_ori])) \n",
    "        mel_list.append(spectrogram.cpu().detach().numpy()[0])\n",
    "        # print(len(mel_list))\n",
    "\n",
    "    mel_concatenated = np.concatenate(mel_list, axis=1)\n",
    "    #! normlized needed\n",
    "    return mel_concatenated\n",
    "\n",
    "\n",
    "\n",
    "def wave2mel_librosa(wavspath, SAMPLING_RATE=22050): #! 아직 노말라이징은 없음\n",
    "    wav_files = glob(os.path.join(\n",
    "        wavspath, '**', '*.wav'), recursive=True)\n",
    "    \n",
    "    wav_files = sorted(wav_files)\n",
    "\n",
    "    print(len(wav_files))\n",
    "    print(wav_files)\n",
    "\n",
    "    mel_list = []\n",
    "\n",
    "    for wav_path in tqdm(wav_files, desc='Preprocessing wav to mel'): \n",
    "        wave_ori, _ = librosa.load(wav_path, sr=SAMPLING_RATE, mono=True)\n",
    "        \n",
    "        spectrogram = librosa.feature.melspectrogram(y=wave_ori, n_fft=1024, n_mels=80, sr=22050, hop_length=256, win_length=1024, fmax=8000, fmin=0, center=False)\n",
    "        mel_list.append(spectrogram)\n",
    "        # print(len(mel_list))\n",
    "\n",
    "    mel_concatenated = np.concatenate(mel_list, axis=1)\n",
    "    #! normlized needed\n",
    "    return mel_concatenated\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataset_melgan(data_path, speaker_id, cache_folder):\n",
    "\n",
    "    \"\"\"Preprocesses dataset of .wav files by converting to Mel-spectrograms.\n",
    "    Args:\n",
    "        data_path (str): Directory containing .wav files of the speaker.\n",
    "        speaker_id (str): ID of the speaker.\n",
    "        cache_folder (str, optional): Directory to hold preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Preprocessing data for speaker: {speaker_id}.\")\n",
    "\n",
    "    mel_concat = wave2mel_melgan(data_path)\n",
    "\n",
    "    if not os.path.exists(os.path.join(cache_folder, speaker_id)):\n",
    "        os.makedirs(os.path.join(cache_folder, speaker_id))\n",
    "    \n",
    "    save_pickle_file(variable = mel_concat, fileName = os.path.join(cache_folder, f\"{speaker_id}_normalized_full.pickle\")) #! 800개 한번에 붙일때\n",
    "\n",
    "    print(f\"Preprocessed and saved data for the speaker: {speaker_id}.\")\n",
    "    \n",
    "\n",
    "def preprocess_dataset_librosa(data_path, speaker_id, cache_folder):\n",
    "\n",
    "    \"\"\"Preprocesses dataset of .wav files by converting to Mel-spectrograms.\n",
    "    Args:\n",
    "        data_path (str): Directory containing .wav files of the speaker.\n",
    "        speaker_id (str): ID of the speaker.\n",
    "        cache_folder (str, optional): Directory to hold preprocessed data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Preprocessing data for speaker: {speaker_id}.\")\n",
    "\n",
    "    mel_concat = wave2mel_librosa(data_path)\n",
    "\n",
    "    if not os.path.exists(os.path.join(cache_folder, speaker_id)):\n",
    "        os.makedirs(os.path.join(cache_folder, speaker_id))\n",
    "    \n",
    "    save_pickle_file(variable = mel_concat, fileName = os.path.join(cache_folder, f\"{speaker_id}_normalized_full.pickle\")) #! 800개 한번에 붙일때\n",
    "\n",
    "    print(f\"Preprocessed and saved data for the speaker: {speaker_id}.\")\n",
    "\n",
    "\n",
    "# 각각저장된 멜 확인\n",
    "\n",
    "\n",
    "def get_mel_spectrogram_fig_melgan(spec, title=\"Mel-Spectrogram\"):\n",
    "    \"\"\"Generates a figure of the Mel-spectrogram and converts it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        spec (torch.Tensor): Mel-spectrogram\n",
    "        title (str, optional): Figure name. Defaults to \"Mel-Spectrogram\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Figure as tensor\n",
    "    \"\"\"\n",
    "    figure, ax = plt.subplots()\n",
    "    canvas = FigureCanvas(figure)\n",
    "    S_db = librosa.power_to_db(10**spec.squeeze(), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, ax=ax, y_axis='log', x_axis='time')\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg')\n",
    "    buf.seek(0)\n",
    "        \n",
    "    image = Image.open(buf)\n",
    "    image = ToTensor()(image)\n",
    "    \n",
    "    plt.close(figure)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def get_mel_spectrogram_fig_librosa(spec, title=\"Mel-Spectrogram\"):\n",
    "    \"\"\"Generates a figure of the Mel-spectrogram and converts it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        spec (torch.Tensor): Mel-spectrogram\n",
    "        title (str, optional): Figure name. Defaults to \"Mel-Spectrogram\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Figure as tensor\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    M = spec\n",
    "    M_db = librosa.power_to_db(M, ref=np.max)\n",
    "    img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "    ax.set(title='Mel spectrogram display')\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "\n",
    "def ndarray_statistics(x):\n",
    "    print(f\"sum: {x.sum()}\")\n",
    "    print(f\"mean: {x.mean()}\")\n",
    "    print(f\"standard deviation: {x.std()}\")\n",
    "    print(f\"variance: {x.var()}\")\n",
    "    print(f\"min: {x.min()}\")\n",
    "    print(f\"max: {x.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .wav 파일 사일런스 제거 Librosa trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(x, y, speaker_id):\n",
    "    file_list = glob(f\"{x}/*\")\n",
    "    idx = 1\n",
    "    print(file_list)\n",
    "    for i in tqdm(file_list):\n",
    "        wave_ori, _ = librosa.load(f\"{i}\", sr=22050, mono=True)\n",
    "        yt, _ = librosa.effects.trim(wave_ori, top_db=30)\n",
    "        torchaudio.save(y + \"/\" + speaker_id + \"_\" + f\"{idx}.wav\", torch.tensor([yt]), sample_rate=22050)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/yuholee/develop/data_KR/eval/A_F_02_e\"\n",
    "save_path = \"/home/yuholee/develop/data_KR_trim/eval/A_F_02_e\"\n",
    "spk_id = \"A_F_02_e\"\n",
    "\n",
    "remove_silence(source_path, save_path, spk_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full_pickle 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"/home/yuholee/develop/data_KR_trim/eval/B_M_14_e\"\n",
    "spk_id = \"B_M_14_e\"\n",
    "save_dir = \"/home/yuholee/develop/data_KR_full_librosa/eval/B_M_14_e\"\n",
    "\n",
    "preprocess_dataset_melgan(source_dir, spk_id, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멜 확인해보기\n",
    "sample_mel = load_pickle_file(\"/home/yuholee/develop/data_KR_full_librosa/eval/B_M_14_e/B_M_14_e_normalized_full.pickle\")\n",
    "print(sample_mel.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "M = sample_mel\n",
    "M_db = librosa.power_to_db(M, ref=np.max)\n",
    "img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "ax.set(title='Mel spectrogram display')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이어진 멜파일을 80간격으로 쪼개서 저장하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_80(load_folder, save_folder, speaker_id):\n",
    "    full_mel = load_pickle_file((f\"{load_folder}\"))\n",
    "\n",
    "    idx = 1\n",
    "    num1 = 0\n",
    "    num2 = 80\n",
    "\n",
    "    while num2 < full_mel[1].shape[0]: #80 단위별로 끊기\n",
    "        print(f\"index{idx}\")\n",
    "\n",
    "        if not os.path.exists(os.path.join(save_folder, speaker_id)):\n",
    "            os.makedirs(os.path.join(save_folder, speaker_id))\n",
    "\n",
    "        save_pickle_file(variable=full_mel[:, num1:num2],\n",
    "                    fileName=os.path.join(save_folder, f\"{speaker_id}_{idx}_normalized.pickle\"))\n",
    "\n",
    "        num1 += 81\n",
    "        num2 += 81\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "load_f = \"/home/yuholee/develop/data_KR_full_librosa/eval/A_F_02_e/A_F_02_e_normalized_full.pickle\"\n",
    "save_f = \"/home/yuholee/develop/data_KR_80_librosa/eval/A_F_02_e\"\n",
    "spk_id = \"A_F_02_e\"\n",
    "\n",
    "mel_80(load_f, save_f, spk_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# librosa로 만든 멜 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mel = load_pickle_file(\"/home/yuholee/develop/data_KR_full_librosa/eval/B_M_14_e/B_M_14_e_normalized_full.pickle\")\n",
    "print(sample_mel.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "M = sample_mel\n",
    "M_db = librosa.power_to_db(M, ref=np.max)\n",
    "img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "ax.set(title='Mel spectrogram display')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80단위로 자른데이터 .wav로 확인해보기 [멜간만]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_to_wav(source_path):\n",
    "    spec_check = load_pickle_file(f\"{source_path}\")\n",
    "    spec_check_tensor = torch.tensor([spec_check])\n",
    "    spec_check_return = vocoder.inverse(spec_check_tensor[0].detach().cpu())\n",
    "    spec_check_return = spec_check_return.cpu()\n",
    "\n",
    "    torchaudio.save(\"/home/yuholee/develop/check_rev_aduio_sample.wav\", spec_check_return, sample_rate=22050)\n",
    "\n",
    "\n",
    "source_file = \"/home/yuholee/develop/data_KR_80_melgan/train/A_F_02_t/A_F_02_t_18_normalized.pickle\"\n",
    "\n",
    "mel_to_wav(source_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멜 확인해보기\n",
    "sample_mel = load_pickle_file(\"/home/yuholee/develop/data_KR_full_melgan/train_mel/A_F_02_t/A_F_02_t_normalized_full.pickle\")\n",
    "print(sample_mel.shape)\n",
    "\n",
    "mel = get_mel_spectrogram_fig_melgan(sample_mel)\n",
    "mel = mel.permute(1,2,0) \n",
    "print(mel.size())\n",
    "plt.imshow(mel)\n",
    "\n",
    "ndarray_statistics(sample_mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그냥 원래음성 보코더 -> 음성합성 해보기 [멜간만]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_ori, _ = librosa.load(\"/home/yuholee/develop/data_KR/eval/A_F_02_e/A_F_02_e_1.wav\", sr=22050, mono=True)\n",
    "\n",
    "spec1 = vocoder(torch.tensor([wave_ori])) \n",
    "spec1 = spec1.cpu()\n",
    "\n",
    "print(type(spec1))\n",
    "print(spec1.shape)\n",
    "print(spec1[0].shape)\n",
    "spec_return1 = vocoder.inverse(spec1[0].detach().cpu())\n",
    "spec_return1 = spec_return1.cpu()\n",
    "\n",
    "torchaudio.save(\"/home/yuholee/develop/ViTCycleGAN/original_aduio_sample.wav\", spec_return1, sample_rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 멜 확인 (멜그리기, 음성변환) [멜간만]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 mel피클파일 넣기\n",
    "\n",
    "converted_mel = load_pickle_file(\"/home/yuholee/develop/ViTCycleGAN/saved_mels/75_mel_x.pickle\")\n",
    "\n",
    "print(type(converted_mel))\n",
    "print(converted_mel.shape)\n",
    "\n",
    "converted_mel = torch.tensor(converted_mel).to(device).float()\n",
    "converted_mel = converted_mel.squeeze(1)\n",
    "\n",
    "\n",
    "            ##############################################\n",
    "            #################    멜그리기 ################\n",
    "            ##############################################\n",
    "\n",
    "mel_conv_temp = get_mel_spectrogram_fig_melgan(converted_mel.cpu()[0].squeeze()) # a: (C, H, W) -> 스퀴즈해서 (80, 80으로)\n",
    "# mel = get_mel_spectrogram_fig(original_mel)\n",
    "mel = mel_conv_temp.permute(1,2,0)  # a: (H, W, C)\n",
    "print(mel.size())\n",
    "plt.imshow(mel)\n",
    "\n",
    "print(type(converted_mel))\n",
    "print(converted_mel.shape)\n",
    "print(converted_mel[0].shape)\n",
    "\n",
    "            ##############################################\n",
    "            ############### 멜 오디오변환 #################\n",
    "            ##############################################\n",
    "\n",
    "spec_return = vocoder.inverse(converted_mel[0].detach().cpu())\n",
    "spec_return = spec_return.cpu()\n",
    "\n",
    "torchaudio.save(\"/home/yuholee/develop/ViTCycleGAN/result_aduio.wav\", spec_return, sample_rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 테스트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Librsoa only (Waveform: librosa, mel: librosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = \"/home/yuholee/develop/ViTCycleGAN/test_sample.wav\"\n",
    "\n",
    "test_wave1, _ = librosa.load(wav_file, sr=22050, mono=True)\n",
    "\n",
    "librosa_mel_test1 = librosa.feature.melspectrogram(y=test_wave1, n_fft=1024, n_mels=80, sr=22050, hop_length=256, win_length=1024, fmax=8000, fmin=0, center=False) \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "M = librosa_mel_test1\n",
    "M_db = librosa.power_to_db(M, ref=np.max)\n",
    "img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "ax.set(title='Mel spectrogram display')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "ndarray_statistics(librosa_mel_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Librosa + MelGAN (Waveform: librosa, mel: MelGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melgan_mel_test1 = vocoder(torch.tensor([test_wave1])) \n",
    "# melgan_mel_test1 = melgan_mel_test1.cpu()\n",
    "# melgan_mel_test1 = melgan_mel_test1.squeeze()\n",
    "# melgan_mel_test1 = melgan_mel_test1.numpy()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# M = melgan_mel_test1\n",
    "# M_db = librosa.power_to_db(M, ref=np.max)\n",
    "# img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "# ax.set(title='Mel spectrogram display')\n",
    "# fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "\n",
    "# ndarray_statistics(melgan_mel_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_conv_temp = get_mel_spectrogram_fig(melgan_mel_test1) \n",
    "# mel = get_mel_spectrogram_fig(original_mel)\n",
    "mel = mel_conv_temp.permute(1,2,0)  \n",
    "plt.imshow(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e78f2cc7a1a588825224f765884fc06aba571ea7626f7989873467d7bf86ca4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
